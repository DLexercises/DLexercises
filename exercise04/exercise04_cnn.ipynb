{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Exercise (Chapter 9)\n",
    "\n",
    "This exercise focuses on convolutional neural networks.\n",
    "We will\n",
    "\n",
    "- implement a convolution layer\n",
    "- extend the convolution with various parameters\n",
    "- apply a convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **reuse the code you wrote in the last exercise, or you can use the code we provide (below)**.\n",
    "We only adjusted the reshaping when loading the data.\n",
    "\n",
    "**Skip forward to the exercises by clicking [here](#exercises)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classes: `Parameter` and `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Optional, Tuple, Callable\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores the value of the parameter but also tensors/\n",
    "    properties associated with it, such as the gradient of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, name: Optional[str] = None):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.name = name\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "\n",
    "\n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def train(self, mode : bool = True) -> 'Module':\n",
    "        \"\"\"Set the module to training mode.\n",
    "\n",
    "        This only affects some Modules, such as Dropout.\n",
    "        \n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def eval(self) -> 'Module':\n",
    "        \"\"\"Set the module to evaluation mode.\n",
    "\n",
    "        This only affects some Modules, such as Dropout.\n",
    "\n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        return self.train(False)\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                return np.sum(self.forward(*input_args))\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                return np.ravel(param.grad)\n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions: `Relu` and `Softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        # Shift input for numerical stability.\n",
    "        reduction_axes = tuple(range(1, len(z.shape)))\n",
    "        shift_z = z - np.max(z, axis=reduction_axes, keepdims=True)\n",
    "        exps = np.exp(shift_z)\n",
    "        h = exps / np.sum(exps, axis=reduction_axes, keepdims=True)\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad) -> np.ndarray:\n",
    "        error_msg = (\"Softmax doesn't need to implement a gradient here, as it's\"\n",
    "                     \"only needed in CrossEntropyLoss, where we can simplify\"\n",
    "                     \"the gradient for the combined expression.\")\n",
    "        raise NotImplementedError(error_msg)\n",
    "\n",
    "\n",
    "class Relu(Module):\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        z = self.input_cache\n",
    "        return grad.reshape(z.shape) * np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(np.random.randn(out_features, in_features) * 0.01,\n",
    "                           name=\"W\")\n",
    "        self.b = Parameter(np.ones((out_features, 1)) * 0.01, name=\"b\")\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "        self.input_cache = x\n",
    "        z = self.W.data @ x + self.b.data\n",
    "        return z\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "        # remember that we have a batch dimension when transposing, i.e.,\n",
    "        # we need to use np.transpose instead of array.T\n",
    "        self.W.grad += np.sum(grad @ np.transpose(x, [0, 2, 1]), axis=0)\n",
    "        self.b.grad += np.sum(grad, axis=0)\n",
    "        return self.W.data.T @ grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return self.W, self.b\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for module in self.modules:\n",
    "            x = module(x)  # equivalent to module.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for module in reversed(self.modules):\n",
    "            grad = module.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]\n",
    "    \n",
    "    def train(self, mode: bool = True) -> 'Sequential':\n",
    "        \"\"\"Set the train mode of the Sequential module and it's sub-modules.\n",
    "        \n",
    "        This only affects some modules, e.g., Dropout.\n",
    "        \n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            module.train(mode)\n",
    "        return self\n",
    "\n",
    "\n",
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2] --> [[0, 1, 0], [0, 0, 2]]\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"The base class for optimizers.\n",
    "\n",
    "    All optimizers must implement a step() method that updates the parameters.\n",
    "    The general optimization loop then looks like this:\n",
    "\n",
    "    for inputs, targets in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    `zero_grad` initializes the gradients of the parameters to zero. This\n",
    "    allows to accumulate gradients (instead of replacing it) during\n",
    "    backpropagation, which is e.g. useful for skip connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: The parameters to be optimized.\n",
    "        \"\"\"\n",
    "        self._params = params\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update the parameters.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "        for param in self._params:\n",
    "            assert isinstance(param, Parameter)\n",
    "            param.grad = np.zeros_like(param.data)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) optimizer with optional Momentum.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float,\n",
    "                 momentum: Optional[float] = None):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        if momentum:\n",
    "            for param in self._params:\n",
    "                param.state_dict[\"momentum\"] = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        for p in self._params:\n",
    "            if self.momentum:\n",
    "                # update the momentum\n",
    "                p.state_dict[\"momentum\"] *= self.momentum\n",
    "                p.state_dict[\"momentum\"] -= self.lr * p.grad\n",
    "                # update the parameter\n",
    "                p.data += p.state_dict[\"momentum\"]\n",
    "            else:\n",
    "                p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        return -np.sum(np.log(a[y == 1])) / len(a)\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg.\n",
    "        a, y = self.input_cache\n",
    "        grad = (a - y) / len(a)\n",
    "\n",
    "        # We have to recreate the batch dimension\n",
    "        grad = np.expand_dims(grad, -1)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, predictions, y_is_onehot: bool = False) -> float:\n",
    "    y_predicted = np.argmax(predictions, axis=-1)\n",
    "    y = np.argmax(y, axis=-1)\n",
    "    return np.sum(np.equal(y_predicted, y)) / len(y)\n",
    "\n",
    "\n",
    "def evaluate(data, labels, model, loss_fn, batch_size):\n",
    "    predictions = []\n",
    "    eval_cost = 0.\n",
    "    data_batched = minibatched(data, batch_size)\n",
    "    labels_batched = minibatched(labels, batch_size)\n",
    "\n",
    "    for x, y in zip(data_batched, labels_batched):\n",
    "        # note that when using cross entropy loss, the softmax is included in the\n",
    "        # loss and we'd need to apply it manually here to obtain the output as probabilities.\n",
    "        # However, softmax only rescales the outputs and doesn't change the argmax,\n",
    "        # so we'll skip this here, as we're only interested in the class prediction.\n",
    "        h_1 = np.squeeze(model(x))\n",
    "        predictions.append(h_1)\n",
    "        eval_cost += loss_fn(h_1, y)\n",
    "    predictions = np.array(predictions).reshape(-1, 10)\n",
    "    eval_accuracy = accuracy(labels, predictions, False)\n",
    "    return eval_accuracy, eval_cost\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, x_train, y_train, x_val, y_val,\n",
    "          num_epochs, batch_size, scheduler=None):\n",
    "    train_costs, train_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    eval_costs, eval_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    ix = np.arange(len(x_train))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {} / {}:\".format(epoch + 1, num_epochs))\n",
    "        training_predictions = []\n",
    "        \n",
    "        np.random.shuffle(ix)\n",
    "        x_train_batched = minibatched(x_train[ix], batch_size)\n",
    "        y_train_batched = minibatched(y_train[ix], batch_size)\n",
    "\n",
    "        # train for one epoch\n",
    "        model.train()\n",
    "        for x_batch, y_batch in zip(x_train_batched, y_train_batched):\n",
    "            optimizer.zero_grad()\n",
    "            y_batch_predicted = model(x_batch)\n",
    "            h_1 = np.squeeze(y_batch_predicted)\n",
    "            training_predictions.append(h_1)\n",
    "            loss = loss_fn(h_1, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            train_costs[epoch] += loss\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        model.eval()\n",
    "    \n",
    "        training_predictions = np.array(training_predictions).reshape(-1, 10)\n",
    "        train_accuracies[epoch] = accuracy(y_train[ix], training_predictions, False)\n",
    "        print(\"  Training Accuracy: {:.4f}\".format(train_accuracies[epoch]))\n",
    "        print(\"  Training Cost: {:.4f}\".format(train_costs[epoch]))\n",
    "        eval_accuracies[epoch], eval_costs[epoch] = evaluate(x_val, y_val, model, loss_fn, batch_size)\n",
    "        print(\"  Eval Accuracy: {:.4f}\".format(eval_accuracies[epoch]))\n",
    "    return train_costs, train_accuracies, eval_costs, eval_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"Loads the data, returns training_data, validation_data, test_data.\"\"\"\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "def minibatched(data: np.ndarray, batch_size: int) -> List[np.ndarray]:\n",
    "    assert len(data) % batch_size == 0, (\"Data length {} is not multiple of batch size {}\"\n",
    "                                         .format(len(data), batch_size))\n",
    "    return data.reshape(-1, batch_size, *data.shape[1:])\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_mnist_data()\n",
    "x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "x_val = x_val.reshape(-1, 1, 28, 28)\n",
    "x_test = x_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = one_hot_encoding(y_train, num_classes)\n",
    "y_val = one_hot_encoding(y_val, num_classes)\n",
    "y_test = one_hot_encoding(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup\n",
    "\n",
    "### Pytorch installation\n",
    "\n",
    " [*PyTorch*](https://pytorch.org/) is a grown-up python library for deep learning. It provides modules like the ones we implemented in the previous exercises and many more. We tried to keep the interface similar, so you will already feel at home when using PyTorch. In contrast to our own implementation, when using pytorch you only have to implement the forward pass - the backward pass will be calculated automagically. \n",
    " \n",
    "In this exercise you have to install pytorch and verify the correct installation. \n",
    "We will use pytorch to verify the correctness of our own convolutional network implementation (next exercise part). In the next exercises, we will use pytorch more and more. \n",
    "\n",
    "**Task:** [Install PyTorch](https://pytorch.org/get-started/locally/) and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 1, 1, 100]), Sequential(\n",
       "   (0): Linear(in_features=100, out_features=30, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=30, out_features=10, bias=True)\n",
       " ), torch.Size([50, 1, 1, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 10))\n",
    "\n",
    "input = torch.randn(50, 1, 1, 100)\n",
    "output = model(input)\n",
    "\n",
    "input.shape, model, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Dataset\n",
    "\n",
    "With convolutions, we finally can see the MNIST data samples as what they are: greyscale images. \n",
    "Instead of flattened features, we load them in 2d shape `x_train.shape == (n_samples, channels, width, height) == (50000, 1, 28, 28)`. \n",
    "\n",
    "**Task:** Plot 20 of the samples as *greyscale* images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPIAAAFFCAYAAADW0LDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJztnXu8jOX2wL/vRojaYud22pFLKSVyq+gid5FbN7lfKkSJcql+JDlHORIpVITqlEOck3JQUiglG6UQkUsq90vktu3398drPTOz9+xtb+adeee1vp+Pz2wzz8ysZ2bWu9az1nrWY9m2jaIo8U1CrAVQFOXcUUVWFB+giqwoPkAVWVF8gCqyovgAVWRF8QGqyHGEZVkdLcv6X5Te6x3Lsp6Nxnsp544q8jlgWdbhoH9plmUdDfp/20i/n23bU23bbhzp1z1XLMtaallWp1jLcT6TO9YCxDO2bReUvy3L2gJ0s23708zGW5aV27bt1GjIppxfqEV2Ecuynrcsa7plWe9ZlvUn0M6yrJssy/rasqwDlmX9blnWWMuy8pwen9uyLNuyrIcty/rZsqz9lmWNDXq9bpZlfZ7Nsbksy3rZsqy9lmVttiyrt2VZmZbxWZZV1bKs1ZZl/WlZ1ntA3qDHiliWNdeyrN2n32eOZVl/O/3YC8BNwITTnsjLp+8fZ1nWr5ZlHbIs61vLsm6O6IerhKCK7D4tgX8BicB0IBV4DEgCagGNgIfTPacJUBWogqP89bJ4/czG9gDqAZWAakCrzF7Asqy8wH+ByUDh03+3CBqSALwBXA6UAk4CYwBs2x4ALAO627Zd0LbtPqef883p9y4MzARmnH4fxQVUkd1nqW3bc2zbTrNt+6ht29/atv2Nbduptm1vBl4Hbkv3nH/Ytn3Qtu0twOdA5SxeP7Ox9wKjbdveYdv2PuCFLF6jFmADr9i2fdK27feBVfKgbdu7bduefVr+Q8Dfw8gcgm3bb9u2ve/0UuJF4GKgXFbPUc4eXSO7z/bg/1iWVQEYhWNFL8T5Dr5J95w/gv7+CyhI5mQ2tmS69w6RIx0lgV/t0B00W4NkLoBjgRsAhU7ffVEWr4dlWf2BLkAJnItEARwvRHEBtcjuk35dOhH4AShn2/bFwGDAcuF9fwcuC/p/cg7GguNGC/2BK4Aap2W+I93YkDlallUH6Au0xlH8S4DDuDNPBVXkWHARcBA4YlnW1WRcH0eKfwN9LMsqaVnWJcCTWYxdCiRYltXrdBDtHuCGdDL/Bey3LKsIzsUnmJ1AmXTjU4E9QB7gWRyLrLiEKnL06Qd0BP7Esc7TXXqf8Thr5jVACvAxcCLcQNu2j+ME5R4E9uMExv4TNOQlnGDdXuArIH1RystAm9OR+JeAucCnwEZgC3AIx+orLmFpY4HzA8uymgEv27ZdNtayKJFHLbJPsSyrgGVZjU7nky/DcYdnx1ouxR3UIvsUy7IKAl8AVwFHgI+APrZt/xlTwRRXUEVWFB+grrWi+ABVZEXxAarIiuIDVJEVxQeoIiuKD1BFVhQfoIqsKD5AFVlRfIAqsqL4AFVkRfEBqsiK4gNUkRXFB6giK4oPUEVWFB+giqwoPkAVWVF8gCqyovgAVWRF8QGqyIriA1SRFcUHqCIrig/I0SFuSUlJdunSpV0SxV1SUlL22LZ9aXbG6jy9j84zlBwpcunSpVmxYsXZSxVDLMvaeuZRDjpP76PzDEVda0XxAarIiuIDVJEVxQeoIiuKD1BFVhQfkKOotRI5tm/fDsCYMWMAGD16NI8//jgAjz32GADJycmxEU6JO2KqyGlpaQAcP348w2NTp04F4MiRI6xduxaAl19+GYCnnnqKcePGAZA/f34ARo0aRY8ePVyX+VzZsWMHAFWqVAHgwIEDAFiWZeYnc9+9e3cMJIw+69ato169egCsXr0agEsvzVaK2NO88cYbAHTv3h1wfu8//fQTAFdeeWVE30tda0XxAa5b5IMHDwJw6tQpAL777jsWLFgABKzR66+/nuVrSFVOv379AJg0aRKJiYkA3HLLLQDccccdkRXcBbZu3crtt98OwP79+wHHEgMkJiaSN29eAHbt2gXA5s2bKVWqFAC5cuWKqqwbN240MtaoUcPV9/rmm2+oW7euq+8RbRYuXEjfvn0BSEgI2Ev5viONWmRF8QGuWuRff/2VypUrAwELlFMSEhKYNGkSEFgPd+3alaJFiwJQsGBBwJtrqpMnTwKOJQZo1KiRCXKlp3LlygwfPhyA2rVrA1C+fHnjrXTt2tVtcUNYuHAh69evB9yzyLZtA47137BhgyvvESs2bNjAsWPHovZ+ripykSJFKFasGHBmRW7QoIF5DsCsWbMAyJs3r3FH440nn3wSwATmsuKLL77gyJEjALRs2RJwPoNVq1a5J2AWjB071nwnbnH48GEA/vGPf5hIvRcvyDlBArPPPvusue+GG24AYMGCBRQoUMCV91XXWlF8gKsWOX/+/EyZMgWAmTNnAnDTTTfRunXrkHG1a9fmv//9LwAXXHABAH/88QcQyLPGG9u3b+edd94BAi4kBKytfAbt2rUDnJzx1VdfDcCAAQMA5zMLfm40keCkm0haBjBzj1d+/vlnAJo0aQLAvn37zGMjRowAMAFaN1CLrCg+wPX0U/Xq1QGoVKkS4Fjc/v37A/Diiy8CMGzYMGOJheLFiwPO+imeCC74CC72AGjbtq0pEpC1lPz//vvv58ILLwSgZMmSgBPoe/vttwEYOHAg4H6112+//RYyDzcJtlr169d3/f3c5M033wQICWa2atUKgDp16rj+/lGr7JIcKcAll1wS8tjYsWNNPtitPJvb7NmzB4AXXngBcIJ7Eui74oorAOjRo4e5YEk0X24z46+//gJg5MiRgPNZuYnk+OV93UCCemvWrDH3SZAz3kj//UjOuEiRIgwbNixqcqhrrSg+ICa11n369AFg+fLlAMyePZsff/wRgGuvvTYWIp0TqampPPHEEwAmwJWYmMj8+fMBKFeuHBDIK58Nv/zyyzlKmT1++OEH8/eZvIWz5emnnwYCbnylSpUyLK3igQMHDtC8efOwjz377LNUqFAharKoRVYUHxATiyxXX6laWrhwobmytWjRAoBatWoBTrrG6+vmbdu2GUssfP311xl2uEhlWrxQs2bNc34N2dmWkpICON/59OnTQ8aMHTuWfPnynfN7RZslS5bw1Vdfhdx3zz33ANCpU6eoyhLTbYyFCxcGYP78+TRq1AgIbFWU28mTJ5ucq5Rjeo1HHnnE5HslTxyJbWppaWkmeBKLfLJE3cMhbnFaWhpffPEFEHD/T5w4wSuvvAIE8tFS0dSgQQOjtLLUiLcc8rfffgtAx44dzX3NmjUDAlmIaF+Y1LVWFB/giQ4hNWrUMMEu6ZIxY8YMALp06cKmTZuAQO3yRRddFAMpMyJ10IsXLzbuv7hWkSAhIcG8brVq1SL2ulkhuWzLsrjrrrsAuOqqqzKMW7ZsGeB4CrlzOz8j8Zhq1qxpgn+SVpTAWYECBUwuXNJQ8VJfLR7KjTfemOExCWi6VUt9JtQiK4oP8IRFBihRogSAqc2WOtx69eqZ7X3SJiV9sCRWyDa148ePm2qsO++886xfLzU1FQgt+rj77rsBp71RNHjuuecAKFu2LJ9//nmm48qXLw/AAw88YKyRFL5kxdy5c00dfTTTM5Fg1KhRQGijAEHq42OFWmRF8QGesciCRPtkD3KuXLmMpfrPf/4DOJY53LotlojcZxtZT01NZfz48QCmFr106dKmeCLaBRMdO3YMicpGio8++sj83aVLl4i/vlvs2LHD7OALpnPnzkDs1/meUWRJZ0hDAQmmiBJDYANGpDsQRoL27duf1fNkc8ILL7zAa6+9BgR+HJLK8CuyqSAeqFatmqmnFxo2bJitphHRQF1rRfEBMbXI0rf51Vdf5a233gKcPl/pkQ6S0k3TK5VeUqRh27YJ0v3f//1ftp773nvvAdC7d2/A2S316KOPAk6zesVb7Nq1K0OQa8CAAZ6pEVeLrCg+IOoW+fDhw8yZMwcIpDqy6qB4xx13mFYpVatWdV/AHCCegWVZxpOQOXXt2tUUrkixy8SJE1myZAkAW7ZsAZw0DziNBcQi+x3xZKS7aJkyZWIpTpZIYYucihKMNMvwAq4rslTvSOeEdu3aZdkZUjo3Dh06FHACXF5xpbNCaopFkSdNmmRqyYM30AuNGzcGMDXmvXr1ioaYnkC+z3DK4RUkCCmR6oSEBNMcY8iQIUDsqrjCoa61ovgAVyzy0aNHAaeBwNKlSwFMs/NwNGnShMGDBwOBmtw8efK4IVpEqVixIuBUn3366achj/36668Z+l4VLVrUHDSX3aCYn/nss88APHlcjPTcDv4OJdga6yqucKhFVhQfEDGLvGXLFv7+978DGOskwYz0yA4baU7Ws2dPz4Txc8LFF18MOOuoadOmAYQNWD3//PMAPPjgg3HbZC6SxKpXt5+JmCJ/8MEH5oymYOS4jDZt2jhvmDs3Dz30EBD9zdduUbBgQXr27AlgbpXwtG7dmgkTJsRajDPyt7/9DQhsgpFMi1dR11pRfEDELHK/fv3M+cWKkhl169b1dNpJkM0vslHH66hFVhQfoIqsKD5AFVlRfIAqsqL4ACsnOT3LsnYD4ZPD3qeUbdvZauOg84wLdJ5B5EiRFUXxJupaK4oPUEVWFB+giqwoPkAVWVF8gCqyovgAVWRF8QGqyIriA1SRFcUH5GgbY1JSki19i+KNlJSUPdmtBNJ5eh+dZyg5UuTSpUuzYsWKs5cqhliWle0SPZ2n99F5hqKutaL4AFVkRfEBqsiK4gNUkRXFB6giK4oPiOn5yIq/kAMHBg8eTI0aNQBYsGABAImJiTGT63xAFTlGHD9+HICTJ08CsHTpUnPOUMeOHQGnmX88cODAAQDGjh0LOCcXpqSkALBt2zYArrvuutgIF0H27NkDQGpqKgDLly+nefPmABkOQQ9H586dmThxIgC5cuWKqGzqWiuKD4iPS75POHDgAKNGjQICJxF+8803GcaJZZYTKr2OnOV11113ATBlypQYShNZ/vjjDwCmTZvG66+/DgTOdd62bZuxxNk5w3vKlClccsklQOA8MDlz+VxRi6woPiCmFnnLli2Ac6WaN28eAN9++23ImHfffZfk5GQAPvnkEwA6depEPNTO7t69G4AxY8aYWzk7WpoeXnHFFQAUKVLErCtlHdWjRw8uvTRb5cQxRU7SlLn4iYEDBwLwzjvvROT1Ro8eDUD37t0BKFu2bEReNyaK/OWXXwJw7733ArBz507zw27VqhUA27dvB6Bdu3bmeTJm9+7dvPrqq1GTNyccO3YMcFyn8ePHA3Dw4MEM4yT488UXXwBOAKVYsWKA83nI8+JBkWXOq1atirEkkadZs2ZAqCKXLFkSgCeeeMK42emDXUuWLGH27NlRklJda0XxBVGzyHLl2rJlizlz9vDhwwC0aNHCLP7Lly8PwKlTpwDo0qUL77//fshr3XzzzVGR+WwQb2PEiBFhH7/mmmsAWLx4MRA4LH3v3r1RkM4dJIW2du3aDI99/fXXAFx++eVA/OWTW7ZsCcC+ffvMfWJ95cTGcDz88MNcffXVQCAFB87vGaBUqVIRlVMtsqL4gKhZ5EWLFgHQsGFDc999990HwOTJkzOE4ZcuXQoQYo0lwCVXSS8SLvVy5ZVXAnDHHXcwfPhwIGCJha1b4/VEE7jooosAePzxxwEnSCfI30WKFAECMZB4Qaxv+u/rTKxcudIUkAQjnkmki31cV2Sp9pEv2bIskx8dMGAAED6X1qdPnwz3TZ8+HQjkLb3Ia6+9BsBNN91Eo0aNAEwQq0CBApk+b9euXe4L5zIPPfQQEKrI5xtigMaMGcNff/2V4fEnn3zSlfdV11pRfICrFnnChAnGEovVvf/++xk0aBAAefLkMWOlfvW7774DYOPGjYCTchKrXq1aNTfFjQjiZvbs2TNHz5NKLz+QlpaWrdpjPyBBy379+gHw448/AnDixIkMY2+55RbXPpfz49NWFJ/jikWWAoFhw4aZGtT7778fcAJb6dm3b58JfElQTHj44Yd58MEH3RAz6sycOZNDhw4BgeIW+Xykqgsw6bkyZcpEWcLIkJCQkK3a43hAdnb9+9//Zu7cuRkenzNnDhC+1rpQoUKAU6cNULt27RAvNJK4osiSA5YKJQiUph05coSZM2cCgeDVsmXLzA9cPhC57datmykBjCdOnjzJb7/9BgQ2PwRXB4WrCJJS1LfeeivDY0p0+f333wG4/fbbAdi0aVOOX0Oqwpo0aRIxuTJDfymK4gNcsciyabp48eJmG1jhwoWB8C7I5ZdfbtwQqbGWlM0NN9zghogRR7yQX3/9FXCu5DIXSZclJyfTuHFjAN577z0gUN0GgYDfxx9/DMADDzwQ8Q3oSs6QJZDcpiezWmsIuNSPPfYYAJUrV3ZDROf9XXtlRVGihisWOV++fICTHL/xxhuBwJa+a665hvbt2wPQoUMHwCmUkPvEisVTUcGpU6dYvXo1ADVr1jT3S3FI3bp1AWfLmmxj/P7774HQxgLivXTu3Blwgl3yevHS9gfCp59kC2q8VHaVKFECCGyrnTFjBg0aNADIMmYzadIkAIYMGeKyhKGoRVYUP2Dbdrb/Va1a1XaDDRs22IAN2AkJCXZCQoI9c+ZMe+bMmRF7D2CFHeF5pqam2qmpqfaoUaPsXLlyhfxr3769ffToUfvo0aNm/JEjR+w6derYderUMfPMnz+/nT9/fnvcuHF2t27d7G7duoW8Tps2bew2bdrY69ats9etW2dv377d/IvWPHNKQkJChs9D/v3xxx8ReQ8vzDMc8p0Hz3nVqlX2qlWrzur1sjtPT/hrx44dy9D7SIJCXkQCHC+//DLg1IxLRZdsmmjYsKFZYsiGiAcffNBUAkljAdkUUqFCBdNZs3fv3oCTc586dSrg5DEFyS9v2LDBhdmdO88884zZHJKeN954g2eeeSbKEkWPlStXxuR91bVWFB/gCYscbz2PP/roIyCwe6tgwYKmwqdq1aoA/PTTT0yYMAEIFIIcPXqUcePGAU5qCUK3x0k9eqVKlQDH4rdu3RpwLJkgxTVeReSPNySFuGbNGipWrAiQ7UosCebdc8897gh3BtQiK4oP8IRFXrNmTaxFyBHpdzalpqby9NNPA4FGez/88EOG540fP56uXbsC2S+/vOWWW0Ju44HWrVubNjfp2//83//9n/n8pEgo1shOu2effRZwSoeltU9WFvno0aMsX74cCOwlCC7wkUIgiZW4iScUefPmzbEWIUdIpxLJ+x47dsz06hLatWtH/fr1gUDgrlChQudN/bSc/bRu3bqQ+704/06dOgGhOX1ZvmTVGWTOnDmmC2r6isVWrVqZrY0VKlSIpLhh8d6nqihKjvGERa5Ro0aWNateY+HChYCzawuczplSCSTbMfPly3de10k/+uijACZ9Fm/IyZLZRXpdS4Xi0KFDo1qN532tURTljHjCIpcoUYJrr70WCKypZC+zF48hkTSR7FWVWyWAxBEkHRfcOMFryL54aSn10ksvZTleepNffPHFpv5aml+IZxZtPKHIEKiSkna5/fv3B2DcuHFmS6MSP0gj+nCnTXqNyy67DIC///3vANx6661069YNCJyJ3KVLF3PapFy4s2pQH23UtVYUH+AZi1y7dm0gcLCb1BYnJSWZ0wzjseWPEj9IcKpp06YmtRgvqEVWFB/gGYssASRpPHfVVVcBThpAKm50rawo4fGMIgui0NJhIdqdFhQlHlHXWlF8gGVn0h0w7GDL2g3E67GBpWzbvjQ7A3WecYHOM4gcKbKiKN5EXWtF8QGqyIriA1SRFcUHqCIrig9QRVYUH6CKrCg+QBVZUXxAjko0k5KSbNkwHm+kpKTsyW4Bgc7T++g8Q8mRIpcuXZoVK1acvVQxxLKsbFf26Dy9j84zFHWtFcUHqCIrig9QRVYUH6CKrCg+QBVZUXyA5zqEKOGR4zpt22bmzJkxliYU6UE+f/58AEaMGMEdd9wBBM6AEtq2bXten8DhFjFVZDmPdtOmTfTp0weAuXPnxlIkzzF8+HAAPv74YwAef/zxWIqTgY8++sic9fznn3+a++WggVdffTVkfI0aNaJyqNn5hrrWiuIDYmqRjx8/DjjHTkq3fzlf1ktd/GPFqFGjjEWWnt533nlnLEXKQN26dc13FWyRM6NWrVrmKFI5Jkg5d9QiK4oP8Eyw69dffwXg4MGDgFpkgKVLl3LixAkAmjVrBsDNN98cS5EykD9/fiZOnAhAmzZtADhy5AhlypQBMh5iv2/fPubMmQOcfxb54MGD5vuUk1Sef/5583jbtm0B+Oc//5nj1/aMIvu5CeDGjRsBGDx4MJMnTwYcBciMJUuWAPDVV1+Zk/9Gjx7tspRnj1xkrr/+esCROykpCcioyADdu3ePnnAxZO3atQC8//77gBP4279/PwCWZWUYL+dunw3qWiuKD/CMRZYrlATA/ITkgNesWcOwYcMAKFeuXKbj+/btC8CuXbuMG1qyZEmXpTx3Ro0aBcATTzzBl19+mem4kydPRkukqDNgwAAAVq5cGdbCynGzvXv3BuCWW26hTp06QOAQubNBLbKi+ADPWGRh9erVACZY4gcuvvhiwPE6JNgRjh07dgCBNXVCQkJceSg33ngjAPPmzaNevXpA+IPOn3nmGQBef/316AnnEkePHgXgueeeA2DkyJEAXHrppeZA9H/84x+A85uWNKJY5kgRU0VOSHAcgksuucQEAaQiyA+88sorACxbtgyAKlWqkFmnihMnTpgvXHLpDRs29FyUOisWL14MOMq7fPnyTMfVrVs3WiK5jiwnXnzxRQCGDh0KOC52NM/zVtdaUXxATC1yvnz5ACd9MW3atFiKEnEOHTrEiBEjAMiTJw8A7777LhdeeGHY8UOHDmXChAkAXH755UB81J3v3r2bBg0aAPDDDz8AkJqamuVzZHy8IUE6WRKMHTuWf/3rXwA0atQIgMqVKwPnFrg6G9QiK4oP8FywK975/fffAahXr57Z3ifrpiuvvDLD+HfffRcIreYZO3as22JGjF9++YX169cDZ7bEgswv3g6xHzduHOCk1wB69OhhimCibYHT4zlF3rNnT6xFyDFpaWksWrQICLiNaWlpJpgnmwSKFy9Ox44dATh27BgAU6ZMAZzKNtmi2LRp06jJfq7UqFGDt99+G4AOHToAgUhuZkh0Pt6Q/L7UPHTu3DnmCiyoa60oPsAbl5Mgpk6dCni7tjg9S5YsoWHDhkDgap2QkEDFihUB+Oyzz8zt9OnTgUCuePv27YBjrSUHGW/cfffdAJQvXx5wAn2CNI9o2bIlAAcOHIiydJFDcuPyfd5zzz2m8k6+61ihFllRfIAnLHKjRo3iMv0k9cT16tUzyf/ChQsD8Omnn3LRRRcBmDZGs2fPNldz2e0lFnznzp1cccUVAKSkpIS8VrwggZ9gZJ6yXa9Xr14sXboUCGxZjXSVUyTYsmULAMnJyQDkypWLDz/8EIC33noLcOqla9euDcBPP/0EQNGiRaMsqYNaZEXxAZ6wyGKJAFOL7OWrtSDr+HLlypmUSv369TOMk7TF0aNHmTdvXtjXsm2bFi1aAPFnibNC1si9evUy9+XNmxcIvyc3lkhp7J133mksrMQ0brvtNrOHvFOnToBjkSUeIM+NlUX2hCIHt0cVVywetrrdd999gFMTLRsjwiFfttRcQ6B5QNmyZc19hQoVckPMmPLSSy9luE/ysFl9ZrFAunseOHDALPVuu+22DOPefPNN8/e9994LwN/+9rcoSJg56lorig/whEWuVq2aqVGVbYziqsr2MC8iDQMyQ4o+pHrrwIEDpnVPPO1qkgKPHj160KVLFwBuvfXWLJ8jrqbs6AqmSZMmEZYwMshv7dFHHzUptWCkx5jUlJcrV87sepLlQqxQi6woPsATFhmgVatWgFO7C06junhHdsZI6qVEiRJZtsDxKtK+ZurUqcZjki6QSUlJJjgnxS1btmxh0KBBQMYCkBEjRpi0nNcQbyNv3rymIULw8Ty7d+8GoF27doCzF7lIkSJRljI8nlFkQSKZ8X4+0MGDB02llsxp0KBBngvwZAfJg2/cuNFE3a+66irAqeaqWbMmgKlykowDBOYuS6fHH3/cM/XJmdG2bVvTmjZeNrCoa60oPsBzl0ZxxaRVjFzt443atWubeurHHnsMgEceeSSWIp010j/ttttuo0ePHgA0b94ccKy0zDMc4nquXLnSZSnPb9QiK4oP8IxFlvYp0v4n3rto9unTh4cffhgIFA3EOwMHDjTNA4Jr48V7kgo2cBoqglriaOEZRZZjR+SLj2YHQjfo2rUrXbt2jbUYEUcCVe3btzf3yd/SNVSJPupaK4oP8IxFTn+yvaIo2UctsqL4AFVkRfEBqsiK4gNUkRXFB1iykT9bgy1rN7DVPXFcpZRt25dmZ6DOMy7QeQaRI0VWFMWbqGutKD5AFVlRfIAqsqL4AFVkRfEBqsiK4gNUkRXFB6giK4oPyNHup6SkJLt06dIuieIuKSkpe7JbQKDz9D46z1BypMilS5dmxYoVZy9VDLEsK9uVPTpP76PzDEVda0XxAZ5pLHC+smfPHgBq1apl+mFt2rQpliIpcYhaZEXxAWqRY8TQoUMBmDBhAuAcR9KhQ4dYiqTEMWqRFcUHqEWOIkeOHDFHsc6fPx8InI1Us2ZNbUConDXnrMgnTpwAYNGiReTPnx/AnDh48OBB0+u4ZcuWAFx22WVhX0dOfJejSC6//PJzFc0zSEDriSeeYMGCBSGPvfXWWwBUr17dfH7xhuxp79WrF+CcB71t2zaAuDy0Lh5R11pRfMA5W+QxY8YAgTN0M0POCj4Tjz/+OADVqlUD4MEHH6R169YAFCpU6GzFjCmHDh0C4J133snwmFQcVahQIZoiRRRJm3388ceAM9+vvvoKgEaNGsVMrvMJtciK4gPO2SLLGi8cRYsW5ZZbbsn08auvvhqAdevWsWvXLgCWLFkCBA4GW758OVWrVgUCh2XHC7I2btymKdcLAAAQsUlEQVS4MRBYSwJ88803QMDziGfy5MkDBOaybds2duzYEUuRoo54nMeOHQNgzZo1GQ5Jr1KlimulouesyEuXLgWcLy99gOqCCy6gYMGC2Xqd48ePA1CxYkUANm/ebB6bMWMGEH+K/N577wGBSq127dqZEwsvuuiimMnlFk8++SQAs2bN4ocffoixNO6xYcMGANauXWuyD2+++SYQerGWjITw/fffc8MNNwCRP6VSXWtF8QHnbJELFy4ccnu2iKsZbInBOS/5oYceOqfXjgWNGzdm8eLFAFx55ZUAvPTSS760xIIslSBQsTZs2DCAbHtmXuHw4cNA4MjY7777zjy2f/9+AP78809jgW+//XYAvvjii0xfMy0tjYMHD7ohrlpkRfEDMa3sOnXqFABDhgxh9OjRYcds2LAh0yISLyLBjAULFpg1Urdu3YBAUMjv2LZtYh6ff/45AE2bNo2hRDlj7dq1tGjRAsjoIabnjz/+AAIeh1jyvXv3mjlv2bLFjL/xxhsjLS4QI0Vev349AJMmTQJg1KhR5jH5sX/wwQcAFC9ePMrSnR0SrVy4cGGGx5KSkoCsq5xmzJiR4Udzpty8VwkO8ohCxxPPPfdcWAXOly8fANOmTQOgatWqXHppaPMOqc575ZVXQhQYnCXWG2+84YLE6lorii+IukXeunUr1113HRBwrYNJSHCuLeJOpw/hexWRU/LfaWlpZi7hcumSmpLnDRkyhJ9//jlkzMCBA01VmJ+DZF5BUmbz5s3L8FjZsmWZO3eu+ftMSK15MB06dODCCy88RynDoxZZUXxA1C3y+++/H9YSC7KmksR5nTp1uO+++wBo1qwZACVKlHBZypyzdu1aAP773/8CjmchV+7gtbFUPMlaesqUKeYxsbplypQBnJSHbHucPn06AImJiW5N4bxn+PDhQCBgBXDnnXcCMGLEiCwtscRIxCP78MMPM7yG7Oxzg6gr8j333GNcmE8//RSAnTt3Zjp+0aJFLFq0CICePXsCzgcuW+YKFCjgprjZ4vjx4xmCI8nJyTz66KMAFClSBHBKNl944QUgUNparFgxwPlcpDLqr7/+Apy8rJSuxhO2bcfNkiiYPn36APDbb7+ZIJZcaM+UB5cSzeCah+rVqwPOts7svMa5oK61oviAqFvkMmXK8PbbbwOYKpdDhw6xb98+IBAEGjlyJBBau5qWlgbAoEGDjAszc+ZMILZBsfXr1xv3Xxg4cCDdu3cHnM4g4DQWkK2M4iLLFfyZZ54xmyzktRITE7nrrrtCxscD8WiNwenSAllXZ6VHaqbFQxTy5MnDwIEDgehUtalFVhQfENPKLrEyiYmJJCcnA3D99dcD0KRJEwCef/55s5YOZvbs2QDGuseyA+Xq1asz3CfWGDABq+A2P19//TUQqMPevHmz+VsYPnx43BaFCJJq9CuyDk7vhXzwwQfmNxwN1CIrig/wbBfNW2+9FXCS89LqR1I7waxbty6qcoVj7969Zi3fuXNnc7+kmn788UfAWe9LdFOsb3DzAXkNGZN+3R2PxEuJ7dkwevRoE7eR4h9BLHW08KwiCwkJCSYIEU6Rr7322miLFBZxrcIFeuRLtizLbKoYNGgQAEePHgWcechjefPmdV1e5eyROogVK1aEfLcQCL5KfX20UNdaUXyA6xZZqmQk7VKpUiVuvvnmbD8/LS0tbFuU3Lkd0WvUqBEBKc+NFi1a0L9/fyBQ6DFo0CDjUgdvJpftmuJGS0HIyJEjfVlPLR02/cDJkycB+OSTT4BAtR0E0k/SNTTaKTi1yIriA1y1yIcPH6Z+/fpAoJWPlB+eCSmiGD9+vFl3BCOdNcuXLx8JUc+JPHnyZNhYXr58+SyvyukLQuKtsWB2kfJaCVjGK8ePH6dv374ATJw40dwvVlnmF6tiGFcVecCAAUaBhb1791K0aFEgtGOGuC3SjfCpp54CQt1ScUcTExOZOnWqe4LnkOTkZNMJQwrvZ82alWFc3759zQWoSpUqABlyx/GM1L1XrVqVlJSUGEsTWQ4ePBiiwADXXHMNd999d4wkCkVda0XxAa5a5KZNmzJ+/PiQ+5KTk81G++A2Kbt37wYCDerDIe7osmXLPGfJxDWWHtznI7ly5QJCmyDMmTMHiF/XWn6XL730krmvUqVKQGDZ4AXUIiuKD3DVIt9000088sgjACFn/2ZlddOTO3dus+6UmmU5+EzxJjVq1DAxg+BN+vGI9OV+7bXXzH1DhgwBvLUjzVVFLlSokMmbSrnhxx9/bKqx/v3vf5ux11xzTchzpatC6dKl46odruJs4ZSNJB07doyxNGeHtLkNDrZKADYndRDRQl1rRfEBrld2SQVW7dq1Q24B2rZt6/bbKzGgUKFCYTtRxhNSiShtesqXL0/v3r0BMvSy9gJqkRXFB6giK0oY7rzzThOnAaeBxaWXXupJawxxsI1RUWKBnCwZL5s+1CIrig+wgrtUnnGwZe0GtronjquUsm07W36RzjMu0HkGkSNFVhTFm6hrrSg+QBVZUXyAKrKi+ABVZEXxAarIiuIDVJEVxQeoIiuKD8hRiWZSUpIdr5v6U1JS9mS3gEDn6X10nqHkSJFLly5tjjWJNyzLynZlj87T++g8Q1HXWlF8gCqyovgAVWRF8QGqyEpU2L9/P/v376dnz55ccMEFXHDBBezevdv0jVbODVVkRfEB2iFEcZXNmzcDcMMNNwBQokQJ0xfaj8fIxoqYKLI0L589ezYAX3/9NatWrQoZ07hxY8A57S5fvnxRlS/WnDhxAnDO2t20aRMA3333HeB0qIwHpK+19IB++umnAejfv3/I4X1KZFDXWlF8QNQsspyL3KtXL3MkauHChQHH+l511VUA/Oc//wECh3/dfPPNrFy5MlpiRo0///wz5BYCx5LKkaSff/45119/PQD58+ePsoRnz969e03/8hYtWgCBUxpidX6w31GLrCg+IGoWWXoEr127lpEjRwLQo0cPINTaSDqifPnyAHz//fdMmDABgO7du0dL3Ijw+++/AzB27FgAtmzZYh4TqytrYIBRo0YBsGbNGsA52F0+h7S0NNflPVekdWy3bt3M2vitt94C/GmJjx49yieffALAY489BsC2bdvM46+//joAXbt2dV0W1xX5xx9/BGDx4sWAo7x9+/bNdLw0AH/++ecBePTRR83ZtPGmyF9++SUAL774YobHJIAnP4BZs2bRr1+/kDGWZZnTLOPBtZbvaf78+eYQtLx588ZSJFeQSHzPnj2NIsuFKviC9fDDDwOwbt06AP75z3+6JpO61oriA1y3yCdPngSgYsWKAHTq1Clbz2vVqhXgWGQJlB0/fhyIj6v8a6+9Rv/+/UPu69u3L8WKFQOcqznAhRdeCEC/fv2oXr06ADt37gSgePHi1KpVK1oinzXiUssSomXLllx88cWxFMkV5HfYpk0bwFkeFS9eHIDOnTsDgTO833jjDbMkXLhwIQCnTp0iV65crsimFllRfIDrFlkONZfgTnaLAYLH/fbbbwAsWLAAgGbNmkVSRFc4fPiwuYKXK1cOcE66L1iwYMi4ffv2AU5MQNaVkoYaP368OZbWy0yePBmAQ4cOATBixIhYiuMad999NxD4Ld9///3m+NX0DB06lJkzZwKBgObOnTspWbKkK7JF7XzknCI55qpVq5oPTgJn8aDI9957LzNmzAAwefDBgwebH7ksEyTwJ6f9AYwZMwaA5s2bR1Xms0Vy/k2bNgUgOTk5luK4RvqL8P3335+t511yySWAuyWp6lorig/wrN8mQYF4CGyF47LLLqNu3bpAwCLPmjXLBEratm0LhOaRX3vtNQBat24dTVHPiY0bNzJv3jwAduzYkem4n376CYDExEQTIIo35Jw0uS1cuLAJ9O3ZsweAadOmAbB8+XJKlSoFBKoV1SIripIlnrXIcqU7cuSIuS+eUhq5c+fOsFNp+/bt3HjjjUDgqi4FBP3796d+/frRFTICTJ06lapVqwKBuAYEUi6yjty7dy/gFML861//AgJ12PHCV199BQS+s2effdZ8j4sWLQoZu3TpUvNdRwPPKvL+/fuBwPY9CGxtDEYiw1u3bmXp0qVA4AciwaNYIdHqrGjXrh3g5JHj6UIlvPjii6ZqTwKbp06dMuW3EgirUqUKACtWrKBBgwYALFu2DIBKlSpFVeazpUSJEgAcPHgQgM8++yzDBVkCYldccUVUZVPXWlF8gGcssrjSkouUjenBiPWqVauWcXNkY8Ivv/xCYmIiAOvXrwcCmxBiQVpamqnDDXeYfPv27QHMls54Q3LeqampGaqVtm3bxn333QeQwb2sVauWqXgbPHgwEAgGeR3pjf3LL78ATl44feWdbJCQCr5ooRZZUXxA1GqtZS27fv16lixZAsDcuXPNONlgL8Uf4Vi+fDlASOfFJ554AnC2Scoa0wvtcHr06MGbb74JhN/CF+/b+iSGAZg0i1CyZEnTSCAcsovNrSont5H1b7gWVIMGDYq2OIBaZEXxBa5YZLHCY8aMMamGcGteoVChQsaaSuRT1swAAwYMAAI7hrxYAigexfTp0wFn94tY3dtuuw2A6tWrmz2pUj/uByQ2IZypiMcLHlMk+Pnnn03Dh4SE2NpEVxS5Q4cOQGgHTAnulC1b1mz1ko0RRYsWNYpcuXJlwOkMAlChQgWGDh0KwAUXXOCGuBFBlgSymRwcZYZAFdeyZcuMIksvrnglfZVTTpCa+fQXgHgjX758RoFl222sNrmoa60oPsCVy8f7778PwJVXXmkqfC677LJMx6elpRlLJbXHknyfP3++py0xOHXE6eujU1JSuO666wBnSyNg2vaA45nEM+Fa22SHU6dOMXr0aCDQ5ije2LVrFwDjxo0zdePSpilWywa1yIriA1yxyHKVLlasWJYpBglode3albfffhsIhPQ/++wzwJuBrfT873//M+mYli1bAk5J4qlTp4DAXPbt22fWlOJxxCvyvSQnJzN//nwA7rrrrkzHy2fx1FNPmWZ0EydOdFnKyHLs2DEAbrrpJsApC5bdTtGsqw6HK4ostbNLly41EWfZ5lW1alWuvvpqIBCNXr16tam/ldxrVq6410hISMjgap46dcrkvSW4l5SUZOYcL00DMkNqiseMGWO2Zr733nsANGzY0OT6N2zYAECfPn0Ap/uJbDCQfmXxwrBhwwBHgcFZKj3wwAOxFMmgrrWi+ABXLLLkjF955RVjgcS1krwyBDpqTp48OW52wIRDul6Ck0oDp7/Thx9+GDLuf//7nzmV0C+0aNGCd999Fwh0lxQXFAIppueeew5wKt7c6iTpJmvXrjUtmMSTkJSqF1CLrCg+wNXsde/evendu7ebb+EJgr0JCeDYtm32Q8suH0lH+Q3Z/x3cBMIvHDhwAAhU5wFmV1u1atViIlM4PLONMZ5p3ry5OeOoV69eANSvX98EubLbbVHxDlJm/PLLLwPOJpGHHnoIgJo1a8ZMrsxQ11pRfIBa5AiQL18+U18ut0p8I1tsJeXUuHFjcySOF1GLrCg+QC2yoqRj69atpg5crHD79u09fXyPdyVTlBhRqlSpkEPp4wF1rRXFB1g52RhuWdZuYKt74rhKKdu2s9XoWucZF+g8g8iRIiuK4k3UtVYUH6CKrCg+QBVZUXyAKrKi+ABVZEXxAarIiuIDVJEVxQeoIiuKD1BFVhQf8P9HHQRZsex8wwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x360 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(data, h=5, w=4, plot_border=True, title=\"\"):\n",
    "    \"\"\" Plot the given image data.\n",
    "    \n",
    "    Args:\n",
    "        h, w: The number of images per col/row.\n",
    "        plot_border: if True, plot the axes, if False don't.\n",
    "        title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # useful functions: plt.subplot, plt.suptitle\n",
    "    # START TODO ################\n",
    "    f, axes = plt.subplots(h, w, subplot_kw={'xticks': [], 'yticks': []}, figsize=(w, h))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            axes[i, j].imshow(data[i*w + j][0], cmap='Greys')\n",
    "    plt.show()\n",
    "    # End TODO ################\n",
    "plot_data(x_train, h=5, w=4, plot_border=False, title=\"Traning data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Output Size\n",
    "\n",
    "The output of the convolution operation depends on the input size ($I$), filter size ($W$), stride ($S$), and padding ($P$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** State the formula for one output dimension.\n",
    "\n",
    "$ O = \\frac{2P + I - W}{S} + 1 $\n",
    "\n",
    "**Task:** Implement the formula for arbitrary dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnd_output(input_shape: Tuple[int], kernel_size: Tuple[int], \n",
    "                  stride: Tuple[int], padding: Tuple[int]) -> Tuple[int]:\n",
    "    assert len(input_shape) == len(kernel_size) == len(stride) == len(padding),\\\n",
    "        \"All inputs need to be of the same length.\"\n",
    "    # output = tuple()\n",
    "    # START TODO ################\n",
    "    output = list(kernel_size)\n",
    "    for i in (0, 1):\n",
    "        output[i] = (2 * padding[i] + input_shape[i] - kernel_size[i]) / stride[i] + 1\n",
    "    # End TODO ################\n",
    "    return tuple(output)\n",
    "\n",
    "\n",
    "np.testing.assert_equal(\n",
    "    convnd_output(input_shape=(11, 11), kernel_size=(3, 3), stride=(1, 1), padding=(0, 0)),\n",
    "    (9, 9))\n",
    "np.testing.assert_equal(\n",
    "    convnd_output(input_shape=(11, 11), kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    (11, 11))\n",
    "np.testing.assert_equal(\n",
    "    convnd_output(input_shape=(11, 11), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "    (6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 4.1\n",
    "\n",
    "This excercise was nice, to finally have a look at the data. And to get used to the convolution operation.\n",
    "**Time**: Approx. 1.5h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLayer Implementation\n",
    "\n",
    "In this exercise, you have to implement your own convolutional layer, **which actually computes cross correlation. This is to ensure compatibility with pytorch.**\n",
    "Don't forget to run the *gradient check*. Additional we verify against the pytorch convolution implementation.\n",
    "Input dimensions for the 2d conv layer are $(\\text{batch}, \\text{channel}, \\text{width}, \\text{height})$.\n",
    "\n",
    "*Tipp 1:* Work iteratively. E.g first implement simple convolution, add stride and padding afterwards. \n",
    "\n",
    "*Tipp 2:* To keep the implementation fast, don't use more than two for-loops in the convolution. [Broadcasting](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) over channels can be helpful here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Module):\n",
    "    \"\"\"2D convolution (cross correlation to be precise).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 kernel_size: Tuple[int], stride: Tuple[int] = (1, 1),\n",
    "                 padding: Tuple[int] = (0, 0)):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pad = padding\n",
    "        w = np.random.randn(out_channels, in_channels, *kernel_size) * 0.1\n",
    "        self.W = Parameter(w, name=\"conv_filter\")\n",
    "        self.b = Parameter(np.zeros((out_channels, 1, 1)), name=\"conv_bias\")\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return (\"Conv2d(in={},out={},\"\n",
    "                \"kernel={},stride={},pad={})\"\n",
    "        .format(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.pad))\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 4\n",
    "        feature_map = np.empty(self._compute_output_shape(x.shape))\n",
    "\n",
    "        # START TODO ################\n",
    "        # Start by gathering some dimension information.\n",
    "        n_x, n_filters, h_out, w_out = self._compute_output_shape(x.shape)\n",
    "        f_h, f_w = self.kernel_size\n",
    "        pad = self.pad[0]\n",
    "        stride = self.stride[0]\n",
    "        \n",
    "        # Convert input into one big 2D array which can be multiplied with the filters..\n",
    "        X_col = Conv2d.im2col_indices(\n",
    "            x,\n",
    "            f_h,\n",
    "            f_w,\n",
    "            padding=pad,\n",
    "            stride=stride\n",
    "        )\n",
    "        W_col = self.W.data.reshape(self.out_channels, -1)\n",
    "        \n",
    "        # Compute the forward pass result. (Filter x Input + bias).\n",
    "        out = W_col @ X_col + self.b.data.reshape(n_filters, 1)\n",
    "        out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "        \n",
    "        # Cache input and converted input -> not have to compute the magic indexing 2 times.\n",
    "        self.input_cache = (x, X_col)\n",
    "\n",
    "        return out\n",
    "        # End TODO ################\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        X, X_col = self.input_cache\n",
    "        h_filter, w_filter = self.kernel_size\n",
    "        \n",
    "        # Gradient w.r.t bias\n",
    "        self.b.grad = np.sum(grad, axis=(0, 2, 3))\n",
    "        self.b.grad = self.b.grad.reshape(self.out_channels, -1, 1)\n",
    "        \n",
    "        # Gradient w.r.t filter weights\n",
    "        grad_reshaped = grad.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        dW = grad_reshaped @ X_col.T\n",
    "        self.W.grad = dW.reshape(self.W.data.shape)\n",
    "        \n",
    "        # Gradient w.r.t input\n",
    "        W_reshape = self.W.data.reshape(self.out_channels, -1)\n",
    "        dX_col = W_reshape.T @ grad_reshaped\n",
    "        dX = Conv2d.col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=self.pad[0], stride=self.stride[0])\n",
    "\n",
    "        return dX\n",
    "        # END TODO ################\n",
    "        \n",
    "    def forward_np_version(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 4\n",
    "        self.input_cache = x\n",
    "\n",
    "        feature_map = np.empty(self._compute_output_shape(x.shape))\n",
    "\n",
    "        x_padded = np.pad(x, ((0, 0), (0, 0), (self.pad[0], self.pad[0]), (self.pad[1], self.pad[1])), mode='constant')\n",
    "\n",
    "        for i in range(0, feature_map.shape[2], self.stride[0]):\n",
    "            for k in range(0, feature_map.shape[3], self.stride[1]):\n",
    "                temp = self.W.data * x_padded[:, :, i:i+self.kernel_size[0], k:k+self.kernel_size[1]][:,np.newaxis,...]\n",
    "                feature_map[:, :, i, k] = np.sum(temp, axis=(2, 3, 4))\n",
    "\n",
    "        feature_map += self.b.data\n",
    "\n",
    "        return feature_map\n",
    "    \n",
    "    def backward_np_version(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "\n",
    "        x_padded = np.pad(x, ((0, 0), (0, 0), (self.pad[0], self.pad[0]), (self.pad[1], self.pad[1])), mode='constant')\n",
    "        x_grad = np.zeros(x_padded.shape)\n",
    "        x_padded = x_padded[:,np.newaxis,...]\n",
    "        # add padding to output image for easier calculation (will be removed later)\n",
    "\n",
    "        # intialize with zeros (just to go sure)\n",
    "        self.W.grad = np.zeros(self.W.grad.shape)\n",
    "        self.b.grad = np.zeros(self.W.grad.shape)\n",
    "\n",
    "        grad_mod = grad[:,:,np.newaxis,...]\n",
    "        W_mod = np.tile(self.W.data, (grad.shape[0],1,1,1,1))\n",
    "\n",
    "        # calculate grad\n",
    "        for i in range(self.kernel_size[0]):\n",
    "            for k in range(self.kernel_size[1]):\n",
    "                x_temp = x_padded[..., \\\n",
    "                                  i:i+x_padded.shape[-2]-self.kernel_size[0]+1:self.stride[0], \\\n",
    "                                  k:k+x_padded.shape[-1]-self.kernel_size[1]+1:self.stride[1]]\n",
    "                self.W.grad[:,:,i,k] += np.sum(x_temp * grad_mod, axis=(0,-2,-1))\n",
    "\n",
    "        for i in range(grad.shape[-2]):\n",
    "            for k in range(grad.shape[-1]):\n",
    "                grad_elem = grad[..., i, k][..., np.newaxis, np.newaxis, np.newaxis]\n",
    "                x_grad[:, :, i*self.stride[0]:i*self.stride[0]+W_mod.shape[-2], \\\n",
    "                             k*self.stride[1]:k*self.stride[1]+W_mod.shape[-1]] += \\\n",
    "                             np.sum(W_mod * grad_elem, axis=1)\n",
    "\n",
    "        # iterate over all convolution layers\n",
    "        self.b.grad = np.sum(grad, axis=(0,2,3))\n",
    "\n",
    "        # remove padding\n",
    "        # be careful, only works for symmetric padding\n",
    "        if np.sum(self.pad) > 0:\n",
    "            x_grad = x_grad[:,:,self.pad[0]:-self.pad[1],self.pad[0]:-self.pad[1]]\n",
    "\n",
    "        # check that size is equal\n",
    "        assert x_grad.shape == x.shape, \"grad w.r.t input size not equal\"\n",
    "\n",
    "        return x_grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return self.b, self.W\n",
    "\n",
    "    def _compute_output_shape(self, input_shape: Tuple[int]) -> Tuple[int]:\n",
    "        \"\"\"Compute the output shape.\"\"\"\n",
    "        [out_h, out_w] = convnd_output(input_shape[2:], self.kernel_size, self.stride, self.pad)\n",
    "        # We never silently pad! Note that usually libraries silently pad for uneven output sizes.\n",
    "        assert out_h % 1 == 0., \"invalid combination of conv parameters\"\n",
    "        assert out_w % 1 == 0., \"invalid combination of conv parameters\"\n",
    "        batch_size = input_shape[0]\n",
    "        return (batch_size, self.out_channels, int(out_h), int(out_w))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "        # First figure out what the size of the output should be\n",
    "        N, C, H, W = x_shape\n",
    "        assert (H + 2 * padding - field_height) % stride == 0\n",
    "        assert (W + 2 * padding - field_height) % stride == 0\n",
    "        out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "        out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "        i0 = np.repeat(np.arange(field_height), field_width)\n",
    "        i0 = np.tile(i0, C)\n",
    "        i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "        j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "        j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "        i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "        j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "        k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "        return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "    @staticmethod\n",
    "    def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "        \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "        # Zero-pad the input\n",
    "        p = padding\n",
    "        x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "        k, i, j = Conv2d.get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "        cols = x_padded[:, k, i, j]\n",
    "        C = x.shape[1]\n",
    "        cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "        return cols\n",
    "\n",
    "    @staticmethod\n",
    "    def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                       stride=1):\n",
    "        \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "        N, C, H, W = x_shape\n",
    "        H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "        x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "        k, i, j = Conv2d.get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "        cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "        cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "        np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "        if padding == 0:\n",
    "            return x_padded\n",
    "        return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "    \n",
    "    \n",
    "class Flatten(Module):\n",
    "    \"\"\" Flatten feature dimension to (batch_size, feature_dim, 1).\n",
    "            \n",
    "        Note: This layer doesn't exist in pytorch but allows us to create\n",
    "              the model using `Sequential` only.\n",
    "    \"\"\"       \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1, 1)\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        old_shape = self.input_cache\n",
    "        return grad.reshape(old_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check implementation for Conv2d(in=3,out=10,kernel=(3, 3),stride=(1, 1),pad=(0, 0)) ... All right!\n",
      "Check implementation for Conv2d(in=3,out=10,kernel=(3, 3),stride=(2, 2),pad=(0, 0)) ... All right!\n",
      "Check implementation for Conv2d(in=3,out=10,kernel=(3, 3),stride=(1, 1),pad=(1, 1)) ... All right!\n",
      "Check implementation for Conv2d(in=3,out=10,kernel=(3, 3),stride=(2, 2),pad=(1, 1)) ... All right!\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def torch_validate_conv2d(conv, x):\n",
    "    x_torch = torch.tensor(x, requires_grad=True)\n",
    "    y_torch = nn.functional.conv2d(x_torch, \n",
    "                                   torch.from_numpy(conv.W.data),\n",
    "                                   stride=conv.stride, padding=conv.pad)\n",
    "    grad_torch = torch.autograd.grad(torch.sum(y_torch), x_torch)\n",
    "    y_pred = conv(x)\n",
    "    grad_pred = conv.backward(np.ones_like(y_pred))\n",
    "    np.testing.assert_allclose(y_pred, y_torch.detach().numpy(), rtol=10e-4,\n",
    "                               err_msg=\"Your convolution forward pass is not equal to pytorch.\")\n",
    "    np.testing.assert_allclose(grad_pred, grad_torch[0].numpy(), rtol=10e-4,\n",
    "                               err_msg=\"Your convolution backward pass is not equal to pytorch.\")\n",
    "\n",
    "\n",
    "x = np.random.rand(2, 3, 11, 11)\n",
    "# standard convb\n",
    "conv = Conv2d(3, 10, (3, 3))\n",
    "# conv with strides\n",
    "conv_stride = Conv2d(3, 10, (3, 3), stride=(2, 2))\n",
    "# conv with padding\n",
    "conv_pad = Conv2d(3, 10, (3, 3), padding=(1, 1))\n",
    "# conv with padding and strides\n",
    "conv_stride_pad = Conv2d(3, 10, (3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "\n",
    "for conv in [conv, conv_stride, conv_pad, conv_stride_pad]:\n",
    "    print(\"Check implementation for {} ... \".format(conv), end=\"\")\n",
    "    conv.check_gradients((x,))\n",
    "    torch_validate_conv2d(conv, x)\n",
    "    print(\"All right!\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Now let's finally train our model. Make sure to set the correct input dimensions. \n",
    "You can reduce the number of samples to train/validate on with `n_samples`, if training takes too long on your device. Of course reducing the sample number also reduces the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10:\n",
      "  Training Accuracy: 0.7187\n",
      "  Training Cost: 188.3383\n",
      "  Eval Accuracy: 0.8955\n",
      "Epoch 2 / 10:\n",
      "  Training Accuracy: 0.9174\n",
      "  Training Cost: 55.9537\n",
      "  Eval Accuracy: 0.9345\n",
      "Epoch 3 / 10:\n",
      "  Training Accuracy: 0.9397\n",
      "  Training Cost: 40.1721\n",
      "  Eval Accuracy: 0.9440\n",
      "Epoch 4 / 10:\n",
      "  Training Accuracy: 0.9510\n",
      "  Training Cost: 32.0945\n",
      "  Eval Accuracy: 0.9492\n",
      "Epoch 5 / 10:\n",
      "  Training Accuracy: 0.9600\n",
      "  Training Cost: 26.5025\n",
      "  Eval Accuracy: 0.9534\n",
      "Epoch 6 / 10:\n",
      "  Training Accuracy: 0.9656\n",
      "  Training Cost: 22.7738\n",
      "  Eval Accuracy: 0.9535\n",
      "Epoch 7 / 10:\n",
      "  Training Accuracy: 0.9697\n",
      "  Training Cost: 20.0961\n",
      "  Eval Accuracy: 0.9624\n",
      "Epoch 8 / 10:\n",
      "  Training Accuracy: 0.9742\n",
      "  Training Cost: 17.5455\n",
      "  Eval Accuracy: 0.9563\n",
      "Epoch 9 / 10:\n",
      "  Training Accuracy: 0.9741\n",
      "  Training Cost: 16.3337\n",
      "  Eval Accuracy: 0.9646\n",
      "Epoch 10 / 10:\n",
      "  Training Accuracy: 0.9785\n",
      "  Training Cost: 14.5504\n",
      "  Eval Accuracy: 0.9634\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "kernel_size = (4, 4)\n",
    "stride = (2, 2)\n",
    "padding = (1, 1)\n",
    "channels_one = 5\n",
    "channels_two = 10\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "n_samples = 10000  # set up to 50000\n",
    "\n",
    "# START TODO ################\n",
    "bs, in_c, in_h, in_w = x_train.shape\n",
    "out_h_l1, out_w_l1 = convnd_output((in_h, in_w), kernel_size, stride, padding)\n",
    "out_h_l2, out_w_l2 = convnd_output((out_h_l1, out_w_l1), kernel_size, stride, padding)\n",
    "conv_model = Sequential(\n",
    "    Conv2d(in_c, channels_one, kernel_size, stride=stride, padding=padding),\n",
    "    Relu(),\n",
    "    Conv2d(channels_one, channels_two, kernel_size, stride=stride, padding=padding),\n",
    "    Relu(),\n",
    "    Flatten(),\n",
    "    Linear( int(out_h_l2*out_w_l2*channels_two), 10)\n",
    ")\n",
    "optimizer = SGD(conv_model.parameters(), learning_rate, momentum)\n",
    "\n",
    "# End TODO ################\n",
    "train(conv_model, loss_fn, optimizer, x_train[:n_samples], y_train[:n_samples],\n",
    "      x_val[:n_samples], y_val[:n_samples], num_epochs=num_epochs, batch_size=batch_size)\n",
    "pass  # don't show output of train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift Invariance\n",
    "Let's now explore an important property of convolutions: shift invariance.\n",
    "For this, we'll shift the validation data by two pixels to the right and bottom.\n",
    "Then we'll compare the accuracies of a standard MLP and the convolution model we\n",
    "trained above on the shifted data.\n",
    "\n",
    "**What do you observe regarding the accuracies? How can the results be explained?**\n",
    "\n",
    "**Answer:**\n",
    "The convNet has a higher accuracy than the vanilla MNP. This is due to the convolution operation, which takes sequences of the input into account and thus will generalize better.\n",
    "\n",
    "**What could we do to improve the accuracy of the MLP model on the shifted validation set?**\n",
    "\n",
    "**Answer:**\n",
    "Eighter just train on the shifted data. But that would require knowledge of the shift.\n",
    "A better improvement would probably be to randomly shift around the training data (Data augmentation) to harden the MLP against translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10:\n",
      "  Training Accuracy: 0.8817\n",
      "  Training Cost: 382.9700\n",
      "  Eval Accuracy: 0.9428\n",
      "Epoch 2 / 10:\n",
      "  Training Accuracy: 0.9501\n",
      "  Training Cost: 167.7406\n",
      "  Eval Accuracy: 0.9523\n",
      "Epoch 3 / 10:\n",
      "  Training Accuracy: 0.9593\n",
      "  Training Cost: 133.2531\n",
      "  Eval Accuracy: 0.9650\n",
      "Epoch 4 / 10:\n",
      "  Training Accuracy: 0.9640\n",
      "  Training Cost: 119.8597\n",
      "  Eval Accuracy: 0.9592\n",
      "Epoch 5 / 10:\n",
      "  Training Accuracy: 0.9672\n",
      "  Training Cost: 108.3227\n",
      "  Eval Accuracy: 0.9618\n",
      "Epoch 6 / 10:\n",
      "  Training Accuracy: 0.9686\n",
      "  Training Cost: 100.0742\n",
      "  Eval Accuracy: 0.9659\n",
      "Epoch 7 / 10:\n",
      "  Training Accuracy: 0.9698\n",
      "  Training Cost: 97.0884\n",
      "  Eval Accuracy: 0.9603\n",
      "Epoch 8 / 10:\n",
      "  Training Accuracy: 0.9726\n",
      "  Training Cost: 88.6779\n",
      "  Eval Accuracy: 0.9607\n",
      "Epoch 9 / 10:\n",
      "  Training Accuracy: 0.9740\n",
      "  Training Cost: 82.8525\n",
      "  Eval Accuracy: 0.9602\n",
      "Epoch 10 / 10:\n",
      "  Training Accuracy: 0.9763\n",
      "  Training Cost: 74.2595\n",
      "  Eval Accuracy: 0.9655\n",
      "Accuracy on shifted data using convolutional model:  0.5592\n",
      "Accuracy on shifted data using an MLP:  0.4896\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "n_samples = 10000  # set up to 50000\n",
    "\n",
    "# START TODO ################\n",
    "# shift the training data by 'shift' pixels to the right and to the bottom\n",
    "# hint: you can use np.pad and slicing\n",
    "shift = 2\n",
    "# First we pad top and left by 'shift'\n",
    "x_val_shifted = np.pad(\n",
    "    x_val,\n",
    "    ((0, 0), (0, 0), (shift, 0), (shift, 0)),\n",
    "    mode='constant'\n",
    ")\n",
    "# Now we cut away the added pad from the right and bottom.\n",
    "x_val_shifted = x_val_shifted[...,0:-shift,0:-shift]\n",
    "# END TODO ################\n",
    "\n",
    "# As always, visualizing your data is important\n",
    "# plot the shifted and non shifted data using the function you defined above\n",
    "plot_data(x_val, h=2, w=4, plot_border=True, title=\"Validation data.\")\n",
    "plot_data(x_val_shifted, h=2, w=4, plot_border=True, title=\"Shifted validation data.\")\n",
    "\n",
    "\n",
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "linear_units = 30\n",
    "# START TODO ################\n",
    "# Train an mlp with the given hyper parameters\n",
    "# Use the non-shifted validation data for evaluation during training,\n",
    "# to verify that the model trains well.\n",
    "# Note that you need to reshape the training and validation data to train an MLP.\n",
    "x_val_shifted_mlp = x_val_shifted.reshape(x_val_shifted.shape[0], -1, 1)\n",
    "x_val_mlp = x_val.reshape(x_val.shape[0], -1, 1)\n",
    "x_train_mlp = x_train.reshape(x_train.shape[0], -1, 1)\n",
    "mlp_model = Sequential(Linear(784,linear_units), Relu(), Linear(linear_units, 10))\n",
    "cel = CrossEntropyLoss()\n",
    "sgd = SGD(mlp_model.parameters(), learning_rate, momentum)\n",
    "train(\n",
    "    mlp_model, \n",
    "    cel, \n",
    "    sgd, \n",
    "    x_train_mlp,\n",
    "    y_train,\n",
    "    x_val_mlp, \n",
    "    y_val,\n",
    "    num_epochs, \n",
    "    batch_size\n",
    ")\n",
    "# End TODO ################\n",
    "\n",
    "print(\"Accuracy on shifted data using convolutional model: \",\n",
    "      evaluate(x_val_shifted, y_val, conv_model, loss_fn, batch_size)[0])\n",
    "print(\"Accuracy on shifted data using an MLP: \",\n",
    "      evaluate(x_val_shifted_mlp, y_val, mlp_model, loss_fn, batch_size)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 4.2\n",
    "\n",
    "This Excercise took us ages to complete (15h). The main struggle was to implement in the back pass the gradient w.r.t the input. While we accomplished it for stride = 1 we gave up after our heads started to smoke because of hardcore numpy.ndarray slicing, reshaping, summing etc (as can be seen in `forward_np_version` and `backward_np_version` respectively). We had no clue at all, how this can be implemented using only two for loops. So we ended up using the `im2col` magic indexing approach implemented in [hipsternet](https://github.com/wiseodd/hipsternet/blob/master/hipsternet/im2col.py) to be able to finish off this exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
